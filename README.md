# NLP-BERT-Classes

åŸºäºBERTçš„ä¸­æ–‡æ–‡æœ¬åˆ†ç±»æ¨¡å‹è®­ç»ƒå·¥å…·ã€‚è¯¥å·¥å…·åˆ©ç”¨é¢„è®­ç»ƒçš„ä¸­æ–‡BERTæ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„å¾®è°ƒè®­ç»ƒã€‚

## é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®æä¾›äº†ä¸€ä¸ªå®Œæ•´çš„ä¸­æ–‡æ–‡æœ¬åˆ†ç±»è§£å†³æ–¹æ¡ˆï¼Œä½¿ç”¨Googleçš„BERT-base-Chineseé¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºåŸºç¡€ï¼Œé€šè¿‡å¾®è°ƒè®­ç»ƒæ¥é€‚åº”ç‰¹å®šé¢†åŸŸçš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚é¡¹ç›®ç‰¹ç‚¹ï¼š

- ä½¿ç”¨é¢„è®­ç»ƒçš„ä¸­æ–‡BERTæ¨¡å‹
- æ”¯æŒå¤šåˆ†ç±»ä»»åŠ¡
- å†…ç½®æ•°æ®é¢„å¤„ç†ä¸å¹³è¡¡é‡‡æ ·åŠŸèƒ½
- æä¾›è¯¦ç»†çš„è®­ç»ƒè¿‡ç¨‹æŒ‡æ ‡ä¸è¯„ä¼°
- æ”¯æŒè‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹

## å®‰è£…æŒ‡å—

### 1. å…‹éš†æ­¤ä»“åº“

```bash
git clone https://github.com/yourusername/nlp-bert-classes.git
cd nlp-bert-classes
```

### 2. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### 3. ä¸‹è½½é¢„è®­ç»ƒBERTæ¨¡å‹

è¯·ä»Hugging Faceä¸‹è½½bert-base-chineseæ¨¡å‹ï¼š

```bash
mkdir -p bert-base-chinese
wget https://huggingface.co/google-bert/bert-base-chinese/resolve/main/pytorch_model.bin -P bert-base-chinese
wget https://huggingface.co/google-bert/bert-base-chinese/resolve/main/config.json -P bert-base-chinese
wget https://huggingface.co/google-bert/bert-base-chinese/resolve/main/vocab.txt -P bert-base-chinese
```

æˆ–è€…ç›´æ¥è®¿é—®: [google-bert/bert-base-chinese](https://huggingface.co/google-bert/bert-base-chinese)

## ä½¿ç”¨è¯´æ˜

### æ•°æ®æ ¼å¼è¦æ±‚

è®­ç»ƒæ•°æ®åº”ä¸ºExcel(.xlsx)æ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹å¿…è¦åˆ—ï¼š
- `question`: å¾…åˆ†ç±»çš„æ–‡æœ¬å†…å®¹
- `TITLE`: æ–‡æœ¬æ ‡é¢˜ï¼ˆå¯é€‰ï¼Œä¼šä¸é—®é¢˜åˆå¹¶ï¼‰
- `label`: åˆ†ç±»æ ‡ç­¾

ç¤ºä¾‹ï¼š

| question | TITLE | label |
|----------|-------|-------|
| è¿™æ˜¯ä¸€ä¸ªé—®é¢˜çš„å†…å®¹ | é—®é¢˜æ ‡é¢˜ | ç±»åˆ«A |
| å¦ä¸€ä¸ªé—®é¢˜çš„å†…å®¹ | å¦ä¸€ä¸ªæ ‡é¢˜ | ç±»åˆ«B |

### è¿è¡Œè®­ç»ƒ

```bash
python bert_train.py --file_path ./data/data.xlsx --model_path ./bert-base-chinese --pt_name my_classifier
```

å‚æ•°è¯´æ˜ï¼š
- `--file_path`: è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º`./data/data.xlsx`
- `--model_path`: BERTé¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼Œé»˜è®¤ä¸º`./bert-base-chinese`
- `--pt_name`: è¾“å‡ºæ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º`nlp-bert-classes`
- `--batch_size`: æ‰¹é‡å¤§å°ï¼Œé»˜è®¤ä¸º1
- `--max_len`: æœ€å¤§æ–‡æœ¬é•¿åº¦ï¼Œé»˜è®¤ä¸º256
- `--epochs`: è®­ç»ƒè½®æ•°ï¼Œé»˜è®¤ä¸º100
- `--lr`: å­¦ä¹ ç‡ï¼Œé»˜è®¤ä¸º5e-5

### è¿è¡Œé¢„æµ‹

è®­ç»ƒå®Œæˆåï¼Œå¯ä»¥ä½¿ç”¨`bert_predict.py`è¿›è¡Œé¢„æµ‹ã€‚é¢„æµ‹æ”¯æŒä¸‰ç§æ¨¡å¼ï¼šæ‰¹é‡é¢„æµ‹æ–‡ä»¶ã€é¢„æµ‹å•æ¡æ–‡æœ¬ã€äº¤äº’å¼é¢„æµ‹ã€‚

#### æ‰¹é‡é¢„æµ‹æ–‡ä»¶

```bash
python bert_predict.py --model_dir ./output/my_classifier_20230101_120000 --input_file ./data/test.xlsx
```

#### é¢„æµ‹å•æ¡æ–‡æœ¬

```bash
python bert_predict.py --model_dir ./output/my_classifier_20230101_120000 --text "è¿™æ˜¯ä¸€ä¸ªéœ€è¦åˆ†ç±»çš„æ–‡æœ¬å†…å®¹"
```

#### äº¤äº’å¼é¢„æµ‹

```bash
python bert_predict.py --model_dir ./output/my_classifier_20230101_120000
```

å‚æ•°è¯´æ˜ï¼š
- `--bert_path`: BERTé¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼Œé»˜è®¤ä¸º`./bert-base-chinese`
- `--model_dir`: è®­ç»ƒå¥½çš„æ¨¡å‹ç›®å½•è·¯å¾„ï¼ˆå¿…éœ€ï¼‰
- `--model_name`: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º`nlp-bert-classes`
- `--weight_type`: ä½¿ç”¨çš„æ¨¡å‹æƒé‡ç±»å‹ï¼Œå¯é€‰`best`æˆ–`last`ï¼Œé»˜è®¤ä¸º`best`
- `--input_file`: è¾“å…¥Excelæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºæ‰¹é‡é¢„æµ‹ï¼‰
- `--text`: è¦é¢„æµ‹çš„å•æ¡æ–‡æœ¬
- `--max_len`: æœ€å¤§æ–‡æœ¬é•¿åº¦ï¼Œé»˜è®¤ä¸º256
- `--batch_size`: æ‰¹å¤„ç†å¤§å°ï¼Œé»˜è®¤ä¸º8

### è¾“å‡ºè¯´æ˜

#### è®­ç»ƒè¾“å‡º

è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹å°†ä¿å­˜åœ¨`output/[pt_name]_[timestamp]`ç›®å½•ä¸‹ï¼š
- `best_[pt_name].pth`: éªŒè¯é›†ä¸Šè¡¨ç°æœ€ä½³çš„æ¨¡å‹æƒé‡
- `last_[pt_name].pth`: æœ€åä¸€è½®è®­ç»ƒçš„æ¨¡å‹æƒé‡
- `[pt_name].pkl`: æ ‡ç­¾ç¼–ç å™¨ï¼ˆç”¨äºé¢„æµ‹æ—¶å°†æ•°å­—æ˜ å°„å›æ ‡ç­¾ï¼‰

#### é¢„æµ‹è¾“å‡º

- æ‰¹é‡é¢„æµ‹ï¼šç»“æœå°†ä¿å­˜åœ¨ä¸è¾“å…¥æ–‡ä»¶åŒç›®å½•ä¸‹çš„`[input_filename]_prediction.xlsx`ä¸­
- å•æ¡æ–‡æœ¬é¢„æµ‹ï¼šç»“æœå°†ç›´æ¥æ‰“å°åˆ°æ§åˆ¶å°
- äº¤äº’å¼é¢„æµ‹ï¼šç»“æœå°†å®æ—¶æ˜¾ç¤ºåœ¨æ§åˆ¶å°

## é«˜çº§é…ç½®

æ›´å¤šé«˜çº§é…ç½®å¯é€šè¿‡ä¿®æ”¹`TrainingConfig`ç±»æ¥å®ç°ï¼ŒåŒ…æ‹¬ï¼š
- æ•°æ®è¿‡æ»¤é˜ˆå€¼
- éªŒè¯é›†æ¯”ä¾‹
- æ—©åœç­–ç•¥
- é‡‡æ ·ç­–ç•¥
- ç­‰ç­‰

## å¼•ç”¨

å¦‚æœæ‚¨ä½¿ç”¨äº†æ­¤é¡¹ç›®ï¼Œè¯·å¼•ç”¨ä»¥ä¸‹èµ„æºï¼š

```
@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
```

## è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ª LICENSE æ–‡ä»¶ä¸­æŒ‡å®šçš„è®¸å¯æ¡æ¬¾ã€‚

## æ„Ÿè°¢ğŸ™
- [google-bert](https://huggingface.co/google-bert): berté¢„è®­ç»ƒæ¨¡å‹ã€‚
- [pytorch](https://pytorch.org/): æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚
